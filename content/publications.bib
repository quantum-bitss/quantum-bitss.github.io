@article{lin2025backdoordm,
  selected={true},
  title={BackdoorDM: A Comprehensive Benchmark for Backdoor Learning in Diffusion Model},
  author={Lin, Weilin^ and Zhou, Nanjun^ and Wang, Yanyun and Li, Jianze and Xiong, Hui and Liu, Li},
  journal={NeurIPS},
  year={2025},
  pdf={https://arxiv.org/abs/2502.11798},
  description={BackdoorDM is the first comprehensive benchmark for backdoor learning on diffusion models, integrating SOTA attacks, defenses, unified evaluation metrics, and multimodal analysis tools to enable fair and systematic comparison.},
  abstract={Backdoor learning is a critical research topic for understanding the vulnerabilities of deep neural networks. While the diffusion model (DM) has been broadly deployed in public over the past few years, the understanding of its backdoor vulnerability is still in its infancy compared to the extensive studies in discriminative models. Recently, many different backdoor attack and defense methods have been proposed for DMs, but a comprehensive benchmark for backdoor learning on DMs is still lacking. This absence makes it difficult to conduct fair comparisons and thoroughly evaluate existing approaches, thus hindering future research progress. To address this issue, we propose BackdoorDM, the first comprehensive benchmark designed for backdoor learning on DMs. It comprises nine state-of-the-art (SOTA) attack methods, four SOTA defense strategies, and three useful visualization analysis tools. We first systematically classify and formulate the existing literature in a unified framework, focusing on three different backdoor attack types and five backdoor target types, which are restricted to a single type in discriminative models. Then, we systematically summarize the evaluation metrics for each type and propose a unified backdoor evaluation method based on multimodal large language model (MLLM). Finally, we conduct a comprehensive evaluation and highlight several important conclusions. We believe that BackdoorDM will help overcome current barriers and contribute to building a trustworthy artificial intelligence generated content (AIGC) community. The codes are released in https://github.com/linweiii/BackdoorDM.},
  keywords={Backdoor learning, Diffusion model, AI security, Benchmark}
}

@inproceedings{zhou2025gradient,
  selected={true},
  title={Gradient norm-based fine-tuning for backdoor defense in automatic speech recognition},
  author={Zhou, Nanjun and Lin, Weilin and Liu, Li},
  booktitle={ICASSP},
  pages={1--5},
  year={2025},
  organization={IEEE},
  pdf={https://arxiv.org/abs/2502.11798},
  description={GN-FT is a gradient-normâ€“based fine-tuning defense that selectively suppresses backdoored neurons, providing the first effective and specialized protection against audio backdoor attacks.},
  abstract={Backdoor attacks have posed a significant threat to the security of deep neural networks (DNNs). Despite considerable strides in developing defenses against backdoor attacks in the visual domain, the specialized defenses for the audio domain remain empty. Furthermore, the defenses adapted from the visual to audio domain demonstrate limited effectiveness. To fill this gap, we propose Gradient Norm-based FineTuning (GN-FT), a novel defense strategy against the attacks in the audio domain, based on the observation from the corresponding backdoored models. Specifically, we first empirically find that the backdoored neurons exhibit greater gradient values compared to other neurons, while clean neurons stay the lowest. On this basis, we fine-tune the backdoored model by incorporating the gradient norm regularization, aiming to weaken and reduce the backdoored neurons. We further approximate the loss computation for lower implementation costs. Extensive experiments on two speech recognition datasets across five models demonstrate the superior performance of our proposed method. To the best of our knowledge, this work is the first specialized and effective defense against backdoor attacks in the audio domain.},
  keywords={AI Security, Backdoor Defense, Automatic Speech Recognition, Fine-Tuning}
}
